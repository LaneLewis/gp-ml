import torch
from tqdm import tqdm
from utils.gaussian_process import sde_kernel_matrices
from utils.dummy_optimizer import DummyOptimizer
from utils.elbo_loss import approx_elbo_loss
from torch import optim
from torch.utils.data import TensorDataset,DataLoader
import gc
import numpy as np
import pickle as pkl
import matplotlib.pyplot as plt

class GPML_VAE():
    def __init__(self,device:str,latent_dims:int,observation_dims:int,times:torch.Tensor,
                 encoder_model:object,decoder_model:object,
                 signal_sds:list[float]=None, noise_sds:list[float]=None,
                 initial_taus:list[float]=None,initial_R_diag:list[float]=None)->object:
        '''
        Implements the GPML framework.
        device - torch device to use
        latent_dims - int specifying the number of latent dims (Z)
        observation_dims - int specifying the number of observation dims (X)
        times - tensor of times specifying the time of each observation
        signal_sds - list of positive floats (size latent_dims) specifying the signal sd of the GP covariance
                    None initializes all parameters to 1.0 - 0.001
        noise_sds - list of positive floats (size latent_dims) specifying the signal sd of the GP covariance
                    None initializes all parameters to 0.001
        initial_taus - list of positive floats (size latent_dims) specifying the time constant of the GP covariance.
                       None initializes the parameters randomly
        initial_R_diag - list of positive floats (size latent_dims) specifying the diagonal noise of the observations (X)
        encoder_model - object with methods: .parameters() which returns the an iterator on the model parameters
                                             .forward(X) -> mu, Sigma. 
                                                This function takes data and returns the mean and covariance of a mv normal distribution.
                                                X has shape [batch_size,timesteps,observation_dims]
                                                mu has shape [batch_size,timesteps,latent_dims]
                                                Sigma has shape [batch_size,timesteps,latent_dims,latent_dims]
        decoder_model - .parameters() which returns the an iterator on the model parameters
                        .forward(Z) -> manifold_mean
                            This function takes in samples from the latent space and returns the mean
                            of the random manifold that the data is generated from.
                            Z has shape [batch_size,samples,timesteps,observation_dims]
        '''
        self.latent_dims = latent_dims
        self.observation_dims = observation_dims
        self.timesteps = times.shape[0]
        #initializes tau
        if initial_taus == None:
            self.taus = torch.rand((latent_dims),requires_grad=False,device=device)
        else:
            assert len(initial_taus) == latent_dims
            self.taus = torch.tensor(initial_taus,requires_grad=False,device=device)
        #initializes R
        if initial_R_diag== None:
            self.R_diag = torch.rand((self.observation_dims),requires_grad=False,device=device)
        else:
            assert len(initial_R_diag) == observation_dims
            self.R_diag = torch.tensor(initial_R_diag,requires_grad=False,device=device)
        #initializes signal sds
        if signal_sds == None:
            self.signal_sds = (1.0 - 0.01)*torch.ones((self.latent_dims),requires_grad=False,device=device)
        else:
            assert len(signal_sds) == latent_dims
            self.signal_sds = torch.tensor(signal_sds,requires_grad=False,device=device)
        #initializes noise_sds
        if noise_sds == None:
            self.noise_sds = 0.01*torch.ones((self.latent_dims),requires_grad=False,device=device)
        else:
            assert len(noise_sds) == latent_dims
            self.noise_sds = torch.tensor(noise_sds,requires_grad=False,device=device)

        #builds all the passed internal variables
        self.encoder_model = encoder_model
        self.decoder_model = decoder_model
        self.times = times
        #constructs the kernel matricies
        
    def fit(self,X_train:torch.Tensor,X_validation:torch.Tensor,encoder_optimizer:object=None,decoder_optimizer:object=None,epochs=100,
            tau_lr=0.001,R_diag_lr =0.001, optimize_taus=True,optimize_R=True,batch_size=1,approx_elbo_loss_samples=100,
            loss_hyperparameter=1.0,log_save_name="latest",hyper_scheduler=None,print_epoch=False,save_cycle=100):
        '''
        X_train- tensor of shape (iid_samples,time_steps,observation_dims): Gives the training data consisting of 
                     data assumed to be generated by independent gaussian process latents with an sde kernel and then 
                     tranformed onto a nonlinear statistical manifold and then sampled from with a normal distribution.
        encoder_optimizer - optimizer object that implements the method .step() which is called after every batch: 
                            optimizer for the encoder model parameters.
        decoder_optimizer - optimizer object that implements the method .step() which is called after every batch:
                            optimizer for the encoder model parameters.
        optimize_taus - bool: indicates whether or not to run the optimizer on taus
        optimize_R - bool: indicates whether or not to run the optimizer on R
        batch_size - int: indicates the batch size to split the data into for each gradient step
        '''
        #constructs an optimizer for the parameters tau and R, makes a dummy optimizer if none is passed
        param_optimize_list = []
        if optimize_taus:
            self.taus.requires_grad = True
            #make run where momentum is higher
            tau_optimizer = optim.Adam([self.taus],lr=tau_lr,amsgrad=True,betas=(0.999,0.999))
        else:
            tau_optimizer = DummyOptimizer()

        if optimize_R:
            self.R_diag.requires_grad = True
            R_optimizer = optim.Adam([self.R_diag],lr=R_diag_lr)
        else:
            R_optimizer = DummyOptimizer()

        #constructs an optimizer for the encoder parameters
        if encoder_optimizer == None:
            for param in self.encoder_model.parameters():
                param.requires_grad = False
            encoder_optimizer = DummyOptimizer()
        
        #constructs an optimizer for the decoder parameters
        if decoder_optimizer == None:
            for param in self.decoder_model.parameters():
                param.requires_grad = False
            decoder_optimizer = DummyOptimizer()

        #begins the main training loop
        batched_X_train = DataLoader(TensorDataset(X_train.detach()),batch_size=batch_size)

        self.epochs_total_loss = []
        self.epochs_ll_loss = []
        self.epochs_kl_loss = []
        self.epochs_taus = []
        self.epochs_R_diags = []
        self.epochs_validation_total_loss = []
        self.epochs_validation_ll_loss = []
        self.epochs_validation_kl_loss = []

        for epoch in tqdm(range(epochs)):
            batch_size = 0
            batch_total_losses = []
            batch_kls = []
            batch_lls = []
            if hyper_scheduler == None:
                loss_hyper = loss_hyperparameter
            else:
                loss_hyper = hyper_scheduler(epoch)

            for (batch_X,) in batched_X_train:
                self.R = torch.diag(self.R_diag)
                self.kernel_matrices = sde_kernel_matrices(self.times,self.taus,self.signal_sds,self.noise_sds)
                encoder_optimizer.zero_grad()
                decoder_optimizer.zero_grad()
                tau_optimizer.zero_grad()
                R_optimizer.zero_grad()
                #computes the loss
                indiv_losses,indiv_kls,indiv_lls = approx_elbo_loss(self.taus,batch_X,self.encoder_model.forward,self.decoder_model.forward,self.R,self.kernel_matrices,
                                                                    samples=approx_elbo_loss_samples,loss_hyper=loss_hyper,taus_to_encoder=False,taus_to_decoder=False)
                batch_loss = torch.mean(indiv_losses)
                batch_kl = torch.mean(indiv_kls.detach())
                batch_ll = torch.mean(indiv_lls.detach())
                batch_loss.backward()

                #steps the grad
                encoder_optimizer.step()
                decoder_optimizer.step()
                tau_optimizer.step()
                R_optimizer.step()
                #training loss
                batch_total_losses.append(batch_loss.clone().detach().numpy())
                batch_kls.append(batch_kl.clone().detach().numpy())
                batch_lls.append(batch_ll.clone().detach().numpy())
                batch_size +=1 

                del batch_kl
                del batch_ll
                del batch_loss
            
            #epoch validation loss
            with torch.no_grad():
                valid_indiv_losses,valid_indiv_kls,valid_indiv_lls = approx_elbo_loss(self.taus,X_validation,self.encoder_model.forward,self.decoder_model.forward,self.R,self.kernel_matrices,
                                                                        samples=approx_elbo_loss_samples,loss_hyper=loss_hyper,taus_to_encoder=False,taus_to_decoder=False)
                valid_loss = torch.mean(valid_indiv_losses)
                valid_kl = torch.mean(valid_indiv_kls)
                valid_ll = torch.mean(valid_indiv_lls)
                self.epochs_validation_total_loss.append(valid_loss.clone().detach().numpy())
                self.epochs_validation_ll_loss.append(valid_ll.clone().detach().numpy())
                self.epochs_validation_kl_loss.append(valid_kl.clone().detach().numpy())

            epoch_kl_loss = (sum(batch_kls)/batch_size)
            epoch_ll_loss = (sum(batch_lls)/batch_size)
            epoch_total_loss = (sum(batch_total_losses)/batch_size)

            epoch_taus = self.taus.clone().detach().numpy()
            epoch_R_diags = self.R_diag.clone().detach().numpy()

            self.epochs_total_loss.append(epoch_total_loss)
            self.epochs_ll_loss.append(epoch_ll_loss)
            self.epochs_kl_loss.append(epoch_kl_loss)

            self.epochs_taus.append(epoch_taus)
            self.epochs_R_diags.append(epoch_R_diags)

            data_struct = {"tau":self.epochs_taus,"R_diag":self.epochs_R_diags,
                           "total_loss":self.epochs_total_loss,"ll_loss":self.epochs_ll_loss,"kl_loss":self.epochs_kl_loss,
                           "validation_loss":self.epochs_validation_total_loss,"validation_kl":self.epochs_kl_loss,"validation_ll":self.epochs_ll_loss}
            if epoch%save_cycle==0:
                save_log(data_struct,log_save_name)
                plot_loss(self.epochs_total_loss,self.epochs_ll_loss,self.epochs_kl_loss,self.epochs_validation_total_loss,log_save_name)
                plot_taus(self.epochs_taus,log_save_name)
            if print_epoch:
                print(f"loss hyper:{loss_hyper}")
                print(f"R_diag: {epoch_R_diags}")
                print(f"Tau: {epoch_taus}")
                print(f"total loss:{epoch_total_loss}")
                print(f"kl loss: {epoch_kl_loss}")
                print(f"ll loss: {epoch_ll_loss}")

            del batch_kls
            del batch_lls
            del batch_total_losses

            gc.collect()

def save_log(data,filename):
    with open(f"./logs/{filename}.pkl","wb") as f:
        pkl.dump(data,f)

def plot_loss(epochs_total_loss,epochs_ll_loss,epochs_kl_loss,epochs_validation_loss,filename):
    epochs = range(len(epochs_total_loss))
    plt.title("GPML VAE: Loss Over Epochs")
    plt.plot(epochs,epochs_total_loss,label="Total Loss",color="black")
    plt.plot(epochs,epochs_ll_loss,label="LL Loss", color = "red")
    plt.plot(epochs,epochs_kl_loss, label="KL Loss",color="blue")
    plt.plot(epochs,epochs_validation_loss,label="Test Loss",color="purple")
    plt.yscale("log")
    plt.ylabel("Loss")
    plt.legend()
    plt.savefig(f"./logs/{filename}_Loss.png")
    plt.cla()

def plot_taus(epochs_taus,filename):
    epochs = range(len(epochs_taus))
    plt.title("GPML VAE: Loss Over Epochs")
    plt.plot(epochs,epochs_taus,label=f"{epochs_taus[-1]}")
    plt.ylabel("Tau")
    plt.legend()
    plt.savefig(f"./logs/{filename}_Taus.png")
    plt.cla()