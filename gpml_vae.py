import torch
from tqdm import tqdm
from utils.gaussian_process import sde_kernel_matrices
from utils.dummy_optimizer import DummyOptimizer
from utils.elbo_loss import approx_elbo_loss
from torch import optim
from torch.utils.data import TensorDataset,DataLoader
import gc

class GPML_VAE():
    def __init__(self,device:str,latent_dims:int,observation_dims:int,times:torch.Tensor,
                 encoder_model:object,decoder_model:object,
                 signal_sds:list[float]=None, noise_sds:list[float]=None,
                 initial_taus:list[float]=None,initial_R_diag:list[float]=None)->object:
        '''
        Implements the GPML framework.
        device - torch device to use
        latent_dims - int specifying the number of latent dims (Z)
        observation_dims - int specifying the number of observation dims (X)
        times - tensor of times specifying the time of each observation
        signal_sds - list of positive floats (size latent_dims) specifying the signal sd of the GP covariance
                    None initializes all parameters to 1.0 - 0.001
        noise_sds - list of positive floats (size latent_dims) specifying the signal sd of the GP covariance
                    None initializes all parameters to 0.001
        initial_taus - list of positive floats (size latent_dims) specifying the time constant of the GP covariance.
                       None initializes the parameters randomly
        initial_R_diag - list of positive floats (size latent_dims) specifying the diagonal noise of the observations (X)
        encoder_model - object with methods: .parameters() which returns the an iterator on the model parameters
                                             .forward(X) -> mu, Sigma. 
                                                This function takes data and returns the mean and covariance of a mv normal distribution.
                                                X has shape [batch_size,timesteps,observation_dims]
                                                mu has shape [batch_size,timesteps,latent_dims]
                                                Sigma has shape [batch_size,timesteps,latent_dims,latent_dims]
        decoder_model - .parameters() which returns the an iterator on the model parameters
                        .forward(Z) -> manifold_mean
                            This function takes in samples from the latent space and returns the mean
                            of the random manifold that the data is generated from.
                            Z has shape [batch_size,samples,timesteps,observation_dims]
        '''
        self.latent_dims = latent_dims
        self.observation_dims = observation_dims
        self.timesteps = times.shape[0]
        #initializes tau
        if initial_taus == None:
            self.taus = torch.rand((latent_dims),requires_grad=False,device=device)
        else:
            assert len(initial_taus) == latent_dims
            self.taus = torch.tensor(initial_taus,requires_grad=False,device=device)
        #initializes R
        if initial_R_diag== None:
            self.R_diag = torch.rand((self.observation_dims),requires_grad=False,device=device)
        else:
            assert len(initial_R_diag) == observation_dims
            self.R_diag = torch.tensor(initial_R_diag,requires_grad=False,device=device)
        #initializes signal sds
        if signal_sds == None:
            self.signal_sds = (1.0 - 0.01)*torch.ones((self.latent_dims),requires_grad=False,device=device)
        else:
            assert len(signal_sds) == latent_dims
            self.signal_sds = torch.tensor(signal_sds,requires_grad=False,device=device)
        #initializes noise_sds
        if noise_sds == None:
            self.noise_sds = 0.01*torch.ones((self.latent_dims),requires_grad=False,device=device)
        else:
            assert len(noise_sds) == latent_dims
            self.noise_sds = torch.tensor(noise_sds,requires_grad=False,device=device)

        #builds all the passed internal variables
        self.encoder_model = encoder_model
        self.decoder_model = decoder_model
        self.times = times
        #constructs the kernel matricies
        self.training_loss = []

    def fit(self,Z,X_train:torch.Tensor,encoder_optimizer:object=None,decoder_optimizer:object=None,epochs=100,
            tau_lr=0.001,R_diag_lr =0.001, optimize_taus=True,optimize_R=True,batch_size=1,approx_elbo_loss_samples=100,loss_hyperparameter=1.0):
        '''
        X_train- tensor of shape (iid_samples,time_steps,observation_dims): Gives the training data consisting of 
                     data assumed to be generated by independent gaussian process latents with an sde kernel and then 
                     tranformed onto a nonlinear statistical manifold and then sampled from with a normal distribution.
        encoder_optimizer - optimizer object that implements the method .step() which is called after every batch: 
                            optimizer for the encoder model parameters.
        decoder_optimizer - optimizer object that implements the method .step() which is called after every batch:
                            optimizer for the encoder model parameters.
        optimize_taus - bool: indicates whether or not to run the optimizer on taus
        optimize_R - bool: indicates whether or not to run the optimizer on R
        batch_size - int: indicates the batch size to split the data into for each gradient step
        '''
        #constructs an optimizer for the parameters tau and R, makes a dummy optimizer if none is passed
        param_optimize_list = []
        if optimize_taus:
            self.taus.requires_grad = True
            tau_optimizer = optim.Adam([self.taus],lr=tau_lr)
        else:
            tau_optimizer = DummyOptimizer()

        if optimize_R:
            self.R_diag.requires_grad = True
            R_optimizer = optim.Adam([self.R_diag],lr=tau_lr)
        else:
            R_optimizer = DummyOptimizer()

        #constructs an optimizer for the encoder parameters
        if encoder_optimizer == None:
            for param in self.encoder_model.parameters():
                param.requires_grad = False
            encoder_optimizer = DummyOptimizer()
        
        #constructs an optimizer for the decoder parameters
        if decoder_optimizer == None:
            for param in self.decoder_model.parameters():
                param.requires_grad = False
            decoder_optimizer = DummyOptimizer()

        #begins the main training loop
        batched_X_train = DataLoader(TensorDataset(X_train),batch_size=batch_size)
        for epoch in range(epochs):
            batch_size = 0
            batch_total_losses = []
            batch_kls = []
            batch_lls = []
            for (batch_X,) in tqdm(batched_X_train,desc=f"epoch: {epoch}",colour="cyan"):
                self.R = torch.diag(self.R_diag)
                self.kernel_matrices = sde_kernel_matrices(self.times,self.taus,self.signal_sds,self.noise_sds)
                encoder_optimizer.zero_grad()
                decoder_optimizer.zero_grad()
                tau_optimizer.zero_grad()
                R_optimizer.zero_grad()
                #computes the loss
                indiv_losses,indiv_kls,indiv_lls = approx_elbo_loss(self.taus,batch_X,self.encoder_model.forward,self.decoder_model.forward,self.R,self.kernel_matrices,
                                                                    samples=approx_elbo_loss_samples,loss_hyper=loss_hyperparameter,taus_to_encoder=False,taus_to_decoder=False)
                batch_loss = torch.sum(indiv_losses)
                batch_kl = torch.sum(indiv_kls)
                batch_ll = torch.sum(indiv_lls)

                batch_loss.backward()
                #steps the grad
                encoder_optimizer.step()
                decoder_optimizer.step()
                tau_optimizer.step()
                R_optimizer.step()
                #training loss
                batch_total_losses.append(batch_loss.clone().detach().numpy())
                batch_kls.append(batch_kl.clone().detach().numpy())
                batch_lls.append(batch_ll.clone().detach().numpy())
                batch_size +=1 
                self.training_loss.append(batch_loss.clone().detach().numpy())
            print(self.decoder_model.parameters())
            print(f"R_diag: {self.R_diag.clone().detach().numpy()}")
            print(f"Tau: {self.taus.clone().detach().numpy()}")
            print(f"total loss:{sum(batch_total_losses)/batch_size}")
            print(f"kl loss: {sum(batch_kls)/batch_size}")
            print(f"ll loss: {sum(batch_lls)/batch_size}")
            gc.collect()